# -*- coding: utf-8 -*-
"""DeepLearning2AIR_MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cMGYMuh4lvgTc5-X87mAIE9jsOFi7Hu5

# Multi-Layer Perceptron (MLP)
auteur : [Maxime Devanne](https://www.maxime-devanne.com)

## Import des librairies
"""

import os
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf

"""## Classification

### Jeu de données Pokemon Stats

#### Téléchargement du dataset
"""

# !wget https://maxime-devanne.com/datasets/Pokemon_stats/pokemon-stats-data.csv

"""#### Lecture du dataset"""

df = pd.read_csv(os.path.join(os.path.dirname(__file__), "pokemon-stats-data.csv"))
df.head()

"""#### Préparation du jeu de données

##### Nettoyage
"""

df = df.dropna()

"""##### Réduction du jeu données en 3 classes
Nous utiliserons une version réduite du jeu de données avec seulement 3 classes :
- fairy
- ground
- flying
"""

df_types = df[(df["type"] !="test")]
types = df_types["type"].unique()

df = df[(df["type"] == "fairy") | (df["type"] == "ground") | (df["type"] == "flying")]

"""##### Extraction des données d'entrée qui seront utilisées

Nous utiliserons les 4 attributs numériques :
- weight
- speed
- sp_attack
- sp_defense
"""

X = np.asarray(df[["weight_kg","speed","sp_attack","sp_defense"]], dtype=np.float32)
print(X.shape)

"""##### Préparation des données de sortie (étiquettes de classes)
Dans le cas d'une classification multi-classes, il faut passer par une représentation en vecteur one-hot.
"""

y = np.asarray(df["type"])

le = LabelEncoder()
y = le.fit_transform(y)

ohe = OneHotEncoder()
y = ohe.fit_transform(y.reshape(-1,1)).toarray()

print(y.shape)
print(y[0])

"""##### Séparation en train-test"""

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=33/100, random_state=42)

"""##### Normalisation des données

Pour reproduire un cas réel, la normalisation est "apprise" sur le jeu d'entrainement seulement et ensuite appliquée sur les jeux de données d'apprentissage et de test.
"""

scaler = MinMaxScaler().fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

"""### Construction d'un MLP

#### Couche d'entrée
Considère des entrées de même taille que celle d'une instance dans `x_train`
"""

input_shape = x_train.shape[1:]
input_layer = tf.keras.layers.Input(input_shape)

"""#### Couche cachée
Contient 16 neurones avec une activation `sigmoid`
"""

# the hidden layer is connected to the input layer
hidden_layer = tf.keras.layers.Dense(units=16, activation='sigmoid')(input_layer)

"""#### Couche de sortie
Contient un nombre de neurones égal au nombre de classes avec une activation `softmax`

"""

nb_classes = y_train.shape[1]
output_layer = tf.keras.layers.Dense(units=nb_classes,activation='softmax')(hidden_layer)

"""#### Création du modèle MLP
On spécifie uniquement la couche d'entrée et la couche de sortie
"""

model_mlp = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)

"""#### Visualisation du modèle construit"""

# textual information
model_mlp.summary()

# connected graph
tf.keras.utils.plot_model(model_mlp, show_shapes=True)

"""#### Compilation du modèle
Il faut notamment spécifier :
- l'algorithme d'optimisation avec un taux d'apprentissage (learning rate)
- la fonction de coût
- une métrique additionnelle (optionnel)
"""

# Optimizer : Adam
learning_rate = 0.001
optimizer_algo = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# Cost function : Categorical Cross-Entropy
cost_function = tf.keras.losses.categorical_crossentropy

# Model compilation with the 'accuracy' as additional metric
model_mlp.compile(loss=cost_function,optimizer=optimizer_algo, metrics=['accuracy'])

"""### Entrainement du MLP
Nous allons d'abord définir quelques hyper-paramètres :
- Nombre d'époques
- Taille de batch
- Pourcentage de données utilisées pour la validation
- Model Checkpoint pour garder le meilleur modèle sur la validation au cours de l'entrainement
"""

nb_epochs = 1000
mini_batch_size = 32
percentage_of_train_as_validation = 0.2
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best-model.h5', monitor='val_loss', save_best_only=True)

"""#### Lancement de l'entrainement
Il est possible de mettre le paramètre `verbose` à `False` afin de ne pas afficher le déroulé de chaque époque

**Soyez patient :)**
"""

history_mlp = model_mlp.fit(x_train,y_train,batch_size=mini_batch_size,
                    epochs=nb_epochs,verbose=True,
                    validation_split=percentage_of_train_as_validation,
                    callbacks=[model_checkpoint])

"""Si vous vérifiez, vous avez à présent un fichier 'best-model.h5' qui est sauvegardé. Il contient votre meilleur modèle entraîné

### Tracer la variation du taux d'erreur sur le train et sur le validation set en fonction du nombre d'epoque
"""

history_dict = history_mlp.history
loss_train_epochs = history_dict['loss']
loss_val_epochs = history_dict['val_loss']

plt.figure()
plt.plot(loss_train_epochs,color='blue',label='train_loss')
plt.plot(loss_val_epochs,color='red',label='val_loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.savefig('epoch-loss.pdf')
plt.show()
plt.close()

"""### Choisir le modèle sauveguardé"""

model = tf.keras.models.load_model('best-model.h5')

"""### Évaluer l'accuracy de ce meilleur modèle sur le test"""

loss,acc = model.evaluate(x_test,y_test)

print("L'accuracy sur l'ensemble du test est:",acc)

"""### **Exercices**

#### Essayez d'améliorer le modèle en ajoutant une couche caché de 16 neurones et comparez les résultats:
"""
input_layer = tf.keras.layers.Input(shape=(x_train.shape[1],))
hidden_layer = tf.keras.layers.Dense(units=16,activation='relu')(input_layer)
hidden_layer = tf.keras.layers.Dense(units=16,activation='relu')(hidden_layer)
output_layer = tf.keras.layers.Dense(units=10, activation='softmax')(hidden_layer)

model_mlp = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)
model_mlp.summary()

"""#### Essayez d'enlever les fonctions d'activation dans les couches cachées (activation='linear') et comparez les résultats"""
input_layer = tf.keras.layers.Input(shape=(x_train.shape[1],))
hidden_layer = tf.keras.layers.Dense(units=16,activation='linear')(input_layer)
output_layer = tf.keras.layers.Dense(units=10, activation='softmax')(hidden_layer)

"""## Régression

Nous allons créer un jeu de données synthétique représentant la fonction non linéaire à estimer par le MLP.
"""

# Create data with equation y=0.1*x*x*sin(x)
x_data = np.linspace(-10, 10, num=1000)
y_data = 0.1*x_data*x_data*np.sin(x_data) + 0.5*np.random.normal(size=1000)

"""Affichage du jeu de données"""

plt.scatter(x_data,y_data)

"""Ajout d'une dimension aux données pour être compatible avec Keras"""

x_data = np.expand_dims(x_data,axis=1)
y_data = np.expand_dims(y_data,axis=1)

"""### Création d'un modèle de regression non linéaire avec Keras

Le modèle contient une couche cachée de 64 neurones.
"""

input_layer = tf.keras.layers.Input(shape=(x_data.shape[1],))
hidden_layer = tf.keras.layers.Dense(units=64,activation='relu')(input_layer)
output_layer = tf.keras.layers.Dense(units=1, activation='linear')(hidden_layer)

multi_layer_model_regression = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)
multi_layer_model_regression.summary()
multi_layer_model_regression.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss='mse')

"""### Entrainement du modèle"""

nb_epochs = 1000
history = multi_layer_model_regression.fit(x_data,y_data,epochs=nb_epochs,verbose=True)

"""Courbe d'erreur sur le train en fonction du nombre d'époques"""

history_dict = history.history
loss_epochs = history_dict['loss']

plt.figure()
plt.plot(loss_epochs)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
plt.close()

"""### Evaluation du modèle"""

loss = multi_layer_model_regression.evaluate(x_data,y_data,verbose=False)

print("Le taux d'erreur sur l'ensemble du test est:",loss)

"""Affichage de la courbe de prédiction"""

# Compute the output
y_predicted = multi_layer_model_regression.predict(x_data)

# Display the result
plt.scatter(x_data[::1], y_data[::1], color='orange')
plt.plot(x_data, y_predicted, 'g', linewidth=1)

"""### **Exercice**

#### Ajoutez une deuxième couche cachée de 64 neurones et comparez les prédictions
"""
input_layer = tf.keras.layers.Input(shape=(x_data.shape[1],))
hidden_layer = tf.keras.layers.Dense(units=64,activation='relu')(input_layer)
hidden_layer = tf.keras.layers.Dense(units=64,activation='relu')(hidden_layer)
output_layer = tf.keras.layers.Dense(units=1, activation='linear')(hidden_layer)